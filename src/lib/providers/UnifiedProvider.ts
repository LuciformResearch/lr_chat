/**
 * Provider Unifi√© pour LR_TchatAgent Web
 * Support OpenRouter + Providers directs (Gemini, OpenAI, Anthropic)
 */

import { HttpResponse } from '@/lib/types/Provider';
import { OpenRouterProvider } from './OpenRouterProvider';
import { OllamaProvider } from './OllamaProvider';
import { sessionApiKeys } from '@/lib/utils/SessionEncryption';

export interface UnifiedProviderConfig {
  type: 'openrouter' | 'custom' | 'ollama';
  provider: 'openrouter' | 'gemini' | 'openai' | 'anthropic' | 'ollama';
  model: string;
  apiKey?: string; // Optionnel pour Ollama
  customConfig?: {
    baseUrl?: string;
    timeout?: number;
  };
}

/**
 * Provider unifi√© qui peut utiliser OpenRouter ou des providers directs
 */
export class UnifiedProvider {
  private config: UnifiedProviderConfig;
  private openRouterProvider?: OpenRouterProvider;
  private ollamaProvider?: OllamaProvider;

  constructor(config: UnifiedProviderConfig) {
    this.config = config;
    
    if (config.type === 'openrouter') {
      this.openRouterProvider = new OpenRouterProvider({
        apiKey: config.apiKey!,
        model: config.model,
        timeout: config.customConfig?.timeout || 30
      });
    } else if (config.type === 'ollama') {
      this.ollamaProvider = new OllamaProvider({
        model: config.model,
        baseUrl: config.customConfig?.baseUrl,
        timeout: config.customConfig?.timeout || 30
      });
    }
  }

  /**
   * V√©rifie si le provider est disponible
   */
  isAvailable(): boolean {
    if (this.config.type === 'ollama') {
      return true; // Ollama sera v√©rifi√© de mani√®re asynchrone
    }

    if (!this.config.apiKey || this.config.apiKey.length < 10) {
      return false;
    }

    if (this.config.type === 'openrouter') {
      return this.openRouterProvider?.isAvailable() || false;
    }

    return true; // Pour les providers directs, on assume que la cl√© est valide
  }

  /**
   * G√©n√®re une r√©ponse via le provider configur√©
   */
  async generateResponse(prompt: string, maxTokens: number = 2000): Promise<HttpResponse> {
    if (this.config.type === 'ollama') {
      // V√©rifier Ollama de mani√®re asynchrone
      const isAvailable = await this.ollamaProvider!.isAvailable();
      if (!isAvailable) {
        return {
          content: '‚ùå Ollama non disponible - v√©rifiez que le service est d√©marr√©',
          error: true
        };
      }
      return await this.ollamaProvider!.generateResponse(prompt, maxTokens);
    }

    if (!this.isAvailable()) {
      return {
        content: '‚ùå Provider non configur√© ou cl√© API manquante',
        error: true
      };
    }

    if (this.config.type === 'openrouter') {
      return await this.openRouterProvider!.generateResponse(prompt, maxTokens);
    }

    // Provider direct
    return await this.generateDirectResponse(prompt, maxTokens);
  }

  /**
   * G√©n√®re une r√©ponse via un provider direct
   */
  private async generateDirectResponse(prompt: string, maxTokens: number): Promise<HttpResponse> {
    try {
      switch (this.config.provider) {
        case 'gemini':
          return await this.generateGeminiResponse(prompt, maxTokens);
        case 'openai':
          return await this.generateOpenAIResponse(prompt, maxTokens);
        case 'anthropic':
          return await this.generateAnthropicResponse(prompt, maxTokens);
        default:
          return {
            content: `‚ùå Provider non support√©: ${this.config.provider}`,
            error: true
          };
      }
    } catch (error) {
      return {
        content: `‚ùå Erreur ${this.config.provider}: ${error}`,
        error: true
      };
    }
  }

  /**
   * G√©n√®re une r√©ponse via Gemini
   */
  private async generateGeminiResponse(prompt: string, maxTokens: number): Promise<HttpResponse> {
    const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent';
    
    const headers = {
      'Content-Type': 'application/json',
      'x-goog-api-key': this.config.apiKey
    };

    const payload = {
      contents: [{
        parts: [{
          text: prompt
        }]
      }],
      generationConfig: {
        maxOutputTokens: maxTokens,
        temperature: 0.7,
        topP: 0.8
      }
    };

    console.log('üåê Appel Gemini:', url);
    console.log('üì§ Model:', this.config.model);

    const response = await fetch(url, {
      method: 'POST',
      headers,
      body: JSON.stringify(payload)
    });

    if (response.ok) {
      const data = await response.json();
      const content = data.candidates?.[0]?.content?.parts?.[0]?.text || 'Pas de r√©ponse';
      
      console.log('‚úÖ R√©ponse Gemini:', content.slice(0, 100) + '...');
      
      return {
        content,
        error: false,
        metadata: {
          model: this.config.model,
          provider: 'gemini'
        }
      };
    } else {
      const errorText = await response.text();
      return {
        content: `Erreur Gemini ${response.status}: ${errorText}`,
        error: true
      };
    }
  }

  /**
   * G√©n√®re une r√©ponse via OpenAI
   */
  private async generateOpenAIResponse(prompt: string, maxTokens: number): Promise<HttpResponse> {
    const url = 'https://api.openai.com/v1/chat/completions';
    
    const headers = {
      'Authorization': `Bearer ${this.config.apiKey}`,
      'Content-Type': 'application/json'
    };

    const payload = {
      model: this.config.model,
      messages: [{
        role: 'user',
        content: prompt
      }],
      max_tokens: maxTokens,
      temperature: 0.7
    };

    console.log('üåê Appel OpenAI:', url);
    console.log('üì§ Model:', this.config.model);

    const response = await fetch(url, {
      method: 'POST',
      headers,
      body: JSON.stringify(payload)
    });

    if (response.ok) {
      const data = await response.json();
      const content = data.choices?.[0]?.message?.content || 'Pas de r√©ponse';
      
      console.log('‚úÖ R√©ponse OpenAI:', content.slice(0, 100) + '...');
      
      return {
        content,
        error: false,
        metadata: {
          model: data.model,
          usage: data.usage,
          provider: 'openai'
        }
      };
    } else {
      const errorText = await response.text();
      return {
        content: `Erreur OpenAI ${response.status}: ${errorText}`,
        error: true
      };
    }
  }

  /**
   * G√©n√®re une r√©ponse via Anthropic
   */
  private async generateAnthropicResponse(prompt: string, maxTokens: number): Promise<HttpResponse> {
    const url = 'https://api.anthropic.com/v1/messages';
    
    const headers = {
      'x-api-key': this.config.apiKey,
      'Content-Type': 'application/json',
      'anthropic-version': '2023-06-01'
    };

    const payload = {
      model: this.config.model,
      max_tokens: maxTokens,
      messages: [{
        role: 'user',
        content: prompt
      }]
    };

    console.log('üåê Appel Anthropic:', url);
    console.log('üì§ Model:', this.config.model);

    const response = await fetch(url, {
      method: 'POST',
      headers,
      body: JSON.stringify(payload)
    });

    if (response.ok) {
      const data = await response.json();
      const content = data.content?.[0]?.text || 'Pas de r√©ponse';
      
      console.log('‚úÖ R√©ponse Anthropic:', content.slice(0, 100) + '...');
      
      return {
        content,
        error: false,
        metadata: {
          model: data.model,
          usage: data.usage,
          provider: 'anthropic'
        }
      };
    } else {
      const errorText = await response.text();
      return {
        content: `Erreur Anthropic ${response.status}: ${errorText}`,
        error: true
      };
    }
  }

  /**
   * Teste la connexion
   */
  async testConnection(): Promise<boolean> {
    try {
      const response = await this.generateResponse('Test de connexion', 10);
      return !response.error;
    } catch {
      return false;
    }
  }

  /**
   * Met √† jour la configuration
   */
  updateConfig(config: Partial<UnifiedProviderConfig>): void {
    this.config = { ...this.config, ...config };
    
    if (config.type === 'openrouter' || config.apiKey) {
      this.openRouterProvider = new OpenRouterProvider({
        apiKey: this.config.apiKey,
        model: this.config.model,
        timeout: this.config.customConfig?.timeout || 30
      });
    }
  }

  /**
   * R√©cup√®re la configuration actuelle
   */
  getConfig(): UnifiedProviderConfig {
    return { ...this.config };
  }
}

/**
 * Factory pour cr√©er des providers unifi√©s
 */
export class UnifiedProviderFactory {
  /**
   * Cr√©e un provider depuis la configuration stock√©e
   */
  static createFromStorage(): UnifiedProvider | null {
    try {
      // V√©rifier si on est c√¥t√© client
      if (typeof window === 'undefined') {
        return null;
      }

      // Utiliser le syst√®me de gestion des cl√©s API
      const providerConfig = localStorage.getItem('lr_tchatagent_provider_config');
      
      if (!providerConfig) {
        console.log('üîë Aucune configuration de provider trouv√©e');
        return null;
      }

      const config = JSON.parse(providerConfig);
      
      // R√©cup√©rer la cl√© API depuis le syst√®me de gestion
      const activeKey = sessionApiKeys.load(config.provider);
      if (!activeKey) {
        console.log(`üîë Aucune cl√© API trouv√©e pour le provider: ${config.provider}`);
        return null;
      }

      console.log(`‚úÖ Provider cr√©√© avec succ√®s: ${config.provider} (${config.model})`);

      return new UnifiedProvider({
        type: config.type,
        provider: config.provider,
        model: config.model,
        apiKey: activeKey,
        customConfig: config.customConfig
      });
    } catch (error) {
      console.error('‚ùå Erreur cr√©ation provider depuis storage:', error);
      return null;
    }
  }

  /**
   * Cr√©e un provider depuis la base de donn√©es PostgreSQL
   */
  static async createFromDatabase(userId: string): Promise<UnifiedProvider | null> {
    try {
      // V√©rifier si on est c√¥t√© client
      if (typeof window === 'undefined') {
        return null;
      }

      console.log(`üîë Chargement des cl√©s API depuis PostgreSQL pour l'utilisateur: ${userId}`);

      // Importer authenticatedGet dynamiquement pour √©viter les probl√®mes c√¥t√© serveur
      const { authenticatedGet } = await import('@/lib/utils/authenticatedFetch');

      // Charger les cl√©s API depuis PostgreSQL avec authentification
      const response = await authenticatedGet('/api/api-keys');
      if (!response.ok) {
        console.error(`‚ùå Erreur chargement cl√©s API: ${response.status}`);
        return null;
      }

      const apiKeys = await response.json();
      console.log('üîë Cl√©s API charg√©es depuis PostgreSQL:', Object.keys(apiKeys));

      // V√©rifier la configuration du provider
      const providerConfig = localStorage.getItem('lr_tchatagent_provider_config');
      if (!providerConfig) {
        console.log('üîë Aucune configuration de provider trouv√©e');
        return null;
      }

      const config = JSON.parse(providerConfig);
      
      // R√©cup√©rer la cl√© API depuis PostgreSQL
      const activeKey = apiKeys[config.provider];
      if (!activeKey) {
        console.log(`üîë Aucune cl√© API trouv√©e dans PostgreSQL pour le provider: ${config.provider}`);
        return null;
      }

      console.log(`‚úÖ Provider cr√©√© avec succ√®s depuis PostgreSQL: ${config.provider} (${config.model})`);

      return new UnifiedProvider({
        type: config.type,
        provider: config.provider,
        model: config.model,
        apiKey: activeKey,
        customConfig: config.customConfig
      });
    } catch (error) {
      console.error('‚ùå Erreur cr√©ation provider depuis PostgreSQL:', error);
      return null;
    }
  }

  /**
   * Cr√©e un provider avec une configuration sp√©cifique
   */
  static create(config: UnifiedProviderConfig): UnifiedProvider {
    return new UnifiedProvider(config);
  }
}